{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa+omrz9tpwLRJKFWSbOA3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JazzLiberal/DataMining/blob/main/FinalModel122108729.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwBULCkKMDrC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA()),\n",
        "    ('select_k_best', SelectKBest()),\n",
        "    ('rf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "#Made a wide and broad param grid to captrure as much as possible, I let it run for several hours to find the best combination.\n",
        "\n",
        "#Best hyper-parameters:  {'pca__n_components': 30, 'pca__whiten': True, 'rf__bootstrap': False, \n",
        "                         #'rf__criterion': 'entropy', 'rf__max_depth': None, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 2, \n",
        "                         ##'rf__n_estimators': 200, 'scaler': StandardScaler(), 'select_k_best__k': 20, 'select_k_best__score_func': <function f_classif at 0x7f99a47d8670>}\n",
        "param_grid = {\n",
        "    'scaler': [StandardScaler()],\n",
        "    'pca__n_components': [30],\n",
        "    'pca__whiten': [True],\n",
        "    'select_k_best__k': [20],\n",
        "    'select_k_best__score_func': [mutual_info_classif],\n",
        "    'rf__n_estimators': [200],\n",
        "    'rf__criterion': ['entropy'],\n",
        "    'rf__max_features': ['sqrt'],\n",
        "    'rf__max_depth': [None],\n",
        "    'rf__min_samples_split': [2],\n",
        "    'rf__min_samples_leaf': [2],\n",
        "    'rf__bootstrap': [False]\n",
        "}\n",
        "\n",
        "# Define the GridSearchCV object - discovered i could use the n_jobs =-1 to allow all my processors cores to be used to try and speed up the process, 5 fold cross validation\n",
        "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1) "
      ]
    }
  ]
}
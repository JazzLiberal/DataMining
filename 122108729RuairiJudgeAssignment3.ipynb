{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS3033/CS6405 - Data Mining - Second Assignment\n",
        "\n",
        "### Submission\n",
        "\n",
        "This assignment is **due on 07/04/22 at 23:59**. You should submit a single .ipnyb file with your python code and analysis electronically via Canvas.\n",
        "Please note that this assignment will account for 25 Marks of your module grade.\n",
        "\n",
        "### Declaration\n",
        "\n",
        "By submitting this assignment. I agree to the following:\n",
        "\n",
        "<font color=\"red\">“I have read and understand the UCC academic policy on plagiarism, and agree to the requirements set out thereby in relation to plagiarism and referencing. I confirm that I have referenced and acknowledged properly all sources used in the preparation of this assignment.\n",
        "I declare that this assignment is entirely my own work based on my personal study. I further declare that I have not engaged the services of another to either assist me in, or complete this assignment”</font>\n",
        "\n",
        "### Objective\n",
        "\n",
        "The Boolean satisfiability (SAT) problem consists in determining whether a Boolean formula F is satisfiable or not. F is represented by a pair (X, C), where X is a set of Boolean variables and C is a set of clauses in Conjunctive Normal Form (CNF). Each clause is a disjunction of literals (a variable or its negation). This problem is one of the most widely studied combinatorial problems in computer science. It is the classic NP-complete problem. Over the past number of decades, a significant amount of research work has focused on solving SAT problems with both complete and incomplete solvers.\n",
        "\n",
        "One of the most successful approaches is an algorithm portfolio, where a solver is selected among a set of candidates depending on the problem type. Your task is to create a classifier that takes as input the SAT instance's features and identifies the class.\n",
        "\n",
        "In this project, we represent SAT problems with a vector of 327 features with general information about the problem, e.g., number of variables, number of clauses, the fraction of horn clauses in the problem, etc. There is no need to understand the features to be able to complete the assignment.\n",
        "\n",
        "\n",
        "The original dataset is available at:\n",
        "https://github.com/bprovanbessell/SATfeatPy/blob/main/features_csv/all_features.csv\n",
        "\n"
      ],
      "metadata": {
        "id": "8WfrCFmLHxYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "Oav9G1WSJ1nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/andvise/DataAnalyticsDatasets/main/train_dataset.csv\", index_col=0)\n",
        "df = df.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE0kM0QsJ1En",
        "outputId": "f2487ed3-aee4-4761-c353-0e8039c4c638"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2412, 310)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label or target variable\n",
        "X = df.iloc[:,:-1]\n",
        "y = df['target']\n"
      ],
      "metadata": {
        "id": "N8MCvTYTKw4Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks\n",
        "\n",
        "## Basic models and evaluation (5 Marks)\n",
        "\n",
        "Using Scikit-learn, train and evaluate a decision tree classifier using 70% of the dataset from training and 30% for testing. For this part of the project, we are not interested in optimising the parameters; we just want to get an idea of the dataset."
      ],
      "metadata": {
        "id": "MTvkBPQvITf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "from sklearn import tree\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = model_selection.train_test_split(X,y, test_size=0.3, random_state=999)\n",
        "\n",
        "cls = tree.DecisionTreeClassifier(random_state = 42)\n",
        "\n",
        "cls.fit(train_features, train_labels)\n",
        "\n",
        "cls.score(test_features, test_labels)"
      ],
      "metadata": {
        "id": "Zl0VXO0YH1nG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e57079-d9a5-4b63-d5f4-a2b694e43b14"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9765193370165746"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robust evaluation (10 Marks)\n",
        "\n",
        "In this section, we are interested in more rigorous techniques by implementing more sophisticated methods, for instance:\n",
        "* Hold-out and cross-validation.\n",
        "* Hyper-parameter tuning.\n",
        "* Feature reduction.\n",
        "* Feature selection.\n",
        "* Feature normalisation.\n",
        "\n",
        "Your report should provide concrete information about your reasoning; everything should be well-explained.\n",
        "\n",
        "The key to geting good marks is to show that you evaluated different methods and that you correctly selected the configuration."
      ],
      "metadata": {
        "id": "zADpr0f8IcGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Data is split into train and test features and labels. 70% Train size set, 30% Test size set in random state 999\n",
        "train_features, test_features, train_labels, test_labels = model_selection.train_test_split(X,y, test_size=0.3, random_state=999)\n",
        "\n",
        "\n",
        "#MinMax scaler, Principle Component Analysis and Decision Tree Classifier are defined and combined into pipeline.\n",
        "scaler = MinMaxScaler()\n",
        "pca = PCA()\n",
        "cls = tree.DecisionTreeClassifier(random_state = 42)\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()), ('pca', PCA()), ('cls', tree.DecisionTreeClassifier())])\n",
        "\n",
        "\n",
        "#Parameters for grid searchCV, list needed to be wide enough to catch best parameters but narrow enough to be practical to run. Parameters changed multiple times, some attempts shown\n",
        "#in cell below\n",
        "#OPTIMAL VALUES -- PCA_n_components: 15 ,  cls_criterion: Entropy,   cls__max_depth: None,     cls__min_samples_split: 2,       pca__whiten: False\n",
        "param_grid = {'pca__n_components': [15],\n",
        "              'pca__whiten': [False],\n",
        "              'cls__criterion': ['entropy'],\n",
        "              'cls__max_depth': [None],\n",
        "              'cls__min_samples_split': [2],\n",
        "              }\n",
        "\n",
        "\n",
        "#Define the GridSearch by passing, the pipeline and the parameter grid, perform 5 fold stratified corss validation.(Data is split again using now using three sets)\n",
        "#Fit/Train the grid on the training data \n",
        "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
        "grid.fit(train_features, train_labels)\n",
        "\n",
        "\n",
        "#Print best classifier, hyperparaters and training score\n",
        "print(\"The best classifier is:\", grid.best_estimator_)\n",
        "print(\"Best hyper-parameters: \", grid.best_params_)\n",
        "print(\"Best score: \", grid.best_score_)\n",
        "\n",
        "\n",
        "\n",
        "y_pred = grid.best_estimator_.predict(test_features)\n",
        "\n",
        "# Compute the accuracy score of the model\n",
        "accuracy = accuracy_score(test_labels, y_pred)\n",
        "print(\"Accuracy score: \", accuracy)\n",
        "\n",
        " \n",
        "\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html \n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
      ],
      "metadata": {
        "id": "tvBZH6ilInsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc89a9e7-ab5c-4b5c-8de2-112c224f9254"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best classifier is: Pipeline(steps=[('scaler', MinMaxScaler()), ('pca', PCA(n_components=15)),\n",
            "                ('cls', DecisionTreeClassifier(criterion='entropy'))])\n",
            "Best hyper-parameters:  {'cls__criterion': 'entropy', 'cls__max_depth': None, 'cls__min_samples_split': 2, 'pca__n_components': 15, 'pca__whiten': False}\n",
            "Best score:  0.952002528400611\n",
            "Accuracy score:  0.93646408839779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'pca__n_components': [20, 25,15,60,45],\n",
        "              'cls__criterion': ['entropy','gini'],\n",
        "              'cls__max_depth': [None],\n",
        "              'cls__min_samples_split': [2,4,10]}\n",
        "\n",
        "param_grid = {'pca__n_components': [20, 25,15,60,45],\n",
        "              'cls__criterion': ['entropy'],\n",
        "              'cls__max_depth': [None],\n",
        "              'cls__min_samples_split': [2]}\n",
        "\n",
        "param_grid = {'pca__n_components': [2, 5, 10, 20, 30, 40],\n",
        "              'cls__criterion': ['gini', 'entropy'],\n",
        "              'cls__max_depth': [None, 5, 10],\n",
        "              'cls__min_samples_split': [2, 5, 10]}\n",
        "\n",
        "param_grid = {'pca__n_components': [15],\n",
        "              'pca__whiten': [True, False],\n",
        "              'cls__criterion': ['entropy'],\n",
        "              'cls__max_depth': [None],\n",
        "              'cls__min_samples_split': [2],\n",
        "              }\n",
        "\n"
      ],
      "metadata": {
        "id": "Q6pNyHWZtRDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New classifier (10 Marks)\n",
        "\n",
        "Replicate the previous task for a classifier different than K-NN and decision trees. Briefly describe your choice.\n",
        "Try to create the best model for the given dataset.\n",
        "\n",
        "\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset:\n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv\n",
        "\n",
        "This link currently contains a sample of the training set. The real test set will be released after the submission. I should be able to run the code cell independently, load all the libraries you need as well."
      ],
      "metadata": {
        "id": "FYoMg0EZIrNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Data Split as before\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = model_selection.train_test_split(X,y, test_size=0.3, random_state=999)\n",
        "\n",
        "#Define a new Pipeline using the RandomForest Classifier \n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA()),\n",
        "    ('select_k_best', SelectKBest()),\n",
        "    ('rf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "#Made a wide and broad param grid to captrure as much as possible, I let it run for several hours to find the best combination.\n",
        "\n",
        "#Best hyper-parameters:  {'pca__n_components': 30, 'pca__whiten': True, 'rf__bootstrap': False, \n",
        "                         #'rf__criterion': 'entropy', 'rf__max_depth': None, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 2, \n",
        "                         ##'rf__n_estimators': 200, 'scaler': StandardScaler(), 'select_k_best__k': 20, 'select_k_best__score_func': <function f_classif at 0x7f99a47d8670>}\n",
        "param_grid = {\n",
        "    'scaler': [StandardScaler()],\n",
        "    'pca__n_components': [30],\n",
        "    'pca__whiten': [True],\n",
        "    'select_k_best__k': [20],\n",
        "    'select_k_best__score_func': [mutual_info_classif],\n",
        "    'rf__n_estimators': [200],\n",
        "    'rf__criterion': ['entropy'],\n",
        "    'rf__max_features': ['sqrt'],\n",
        "    'rf__max_depth': [None],\n",
        "    'rf__min_samples_split': [2],\n",
        "    'rf__min_samples_leaf': [2],\n",
        "    'rf__bootstrap': [False]\n",
        "}\n",
        "\n",
        "# Define the GridSearchCV object - discovered i could use the n_jobs =-1 to allow all my processors cores to be used to try and speed up the process, 5 fold cross validation\n",
        "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "grid.fit(train_features, train_labels)\n",
        "\n",
        "#print the best combination\n",
        "print(\"The best classifier is:\", grid.best_estimator_)\n",
        "print(\"Best hyper-parameters: \", grid.best_params_)\n",
        "print(\"Best score: \", grid.best_score_)\n",
        "\n",
        "y_pred = grid.best_estimator_.predict(test_features)\n",
        "\n",
        "# Compute the accuracy score of the model\n",
        "accuracy = accuracy_score(test_labels, y_pred)\n",
        "print(\"Accuracy score: \", accuracy)     \n",
        "\n",
        "\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier"
      ],
      "metadata": {
        "id": "QRJXrY2hI32F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e165b5-3c86-4634-a83a-0bdfce8aedad"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best classifier is: Pipeline(steps=[('scaler', StandardScaler()),\n",
            "                ('pca', PCA(n_components=30, whiten=True)),\n",
            "                ('select_k_best',\n",
            "                 SelectKBest(k=20,\n",
            "                             score_func=<function mutual_info_classif at 0x7f99a47ea280>)),\n",
            "                ('rf',\n",
            "                 RandomForestClassifier(bootstrap=False, criterion='entropy',\n",
            "                                        min_samples_leaf=2,\n",
            "                                        n_estimators=200))])\n",
            "Best hyper-parameters:  {'pca__n_components': 30, 'pca__whiten': True, 'rf__bootstrap': False, 'rf__criterion': 'entropy', 'rf__max_depth': None, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 2, 'rf__n_estimators': 200, 'scaler': StandardScaler(), 'select_k_best__k': 20, 'select_k_best__score_func': <function mutual_info_classif at 0x7f99a47ea280>}\n",
            "Best score:  0.9905220093761523\n",
            "Accuracy score:  0.9834254143646409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Original_param_grid = {\n",
        "    'scaler': [StandardScaler(), MinMaxScaler()],\n",
        "    'pca__n_components': [10, 24, 30],\n",
        "    'pca__whiten': [True, False],\n",
        "    'select_k_best__k': [5, 10, 20],\n",
        "    'select_k_best__score_func': [f_classif, mutual_info_classif],\n",
        "    'rf__n_estimators': [100, 200, 500],\n",
        "    'rf__criterion': ['entropy'],\n",
        "    'rf__max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'rf__max_depth': [None],\n",
        "    'rf__min_samples_split': [2, 5, 10],\n",
        "    'rf__min_samples_leaf': [2],\n",
        "    'rf__bootstrap': [True, False]\n",
        "}"
      ],
      "metadata": {
        "id": "iW0b3I0gGUPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IeEwmEuy_RXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">FOR GRADING ONLY</font>\n",
        "\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset: \n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv\n"
      ],
      "metadata": {
        "id": "Q01BjiiCJTR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "mLink = 'https://github.com/JazzLiberal/DataMining/blob/main/model122108729.joblib?raw=true'\n",
        "mfile = BytesIO(requests.get(mLink).content)\n",
        "model = load(mfile)\n",
        "\n",
        "# Load the dataset and replicate your preprocessing\n",
        "df = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv?raw=true\", index_col=0)\n",
        "df = df.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
        "\n",
        "\n",
        "X = df.iloc[:,:-1]\n",
        "y = df['target']\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "#Data Split as before\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = model_selection.train_test_split(X,y, test_size=0.3, random_state=122)\n",
        "\n",
        "# Evaluate your model or pipeline\n",
        "model.score(test_features, test_labels)  "
      ],
      "metadata": {
        "id": "IWx4lyuQI929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "5ce68e8a-d479-45c8-a448-b90690852bb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e1b42630a97d>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmLink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://raw.githubusercontent.com/JazzLiberal/DataMining/main/FinalModel122108729.ipynb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
            "\u001b[0;32m/usr/lib/python3.9/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 123"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w-k6H-1ZKTYo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}